<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Datasets that Cause Overfitting on Decision Trees but Perform Well on Random Forests</title>
    <script>
        // JavaScript to handle the counter
        if (localStorage.getItem("counter") === null) {
            localStorage.setItem("counter", 0); // Initialize counter if not present
        }

        function incrementCounter() {
            let counter = parseInt(localStorage.getItem("counter"));
            counter++;
            localStorage.setItem("counter", counter); // Update counter in local storage
            document.getElementById("counterDisplay").innerText = counter;
        }
    </script>
    <style>
        #counterSection {
            background-color: #f2f2f2;
            color: #333;
            padding: 10px;
            text-align: center;
            margin-top: 30px;
            font-size: 1.2em;
            border-top: 2px solid #ddd;
        }
    </style>
</head>
<body onload="incrementCounter()">
    <header>
        <h1>Datasets that Cause Overfitting on Decision Trees but Perform Well on Random Forests</h1>
    </header>
    <section>
        <p>Overfitting in decision trees occurs when the model is too complex and captures noise in the data, leading to poor generalization. However, random forests tend to handle this better because they aggregate results from multiple decision trees, reducing overfitting by averaging out predictions from different models. Below are some datasets where decision trees may overfit, but random forests give better results:</p>

        <h2>1. Iris Dataset</h2>
        <p><strong>Description:</strong> A classic dataset with 150 samples and 4 features, often used for classification tasks.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Decision trees may overfit due to the simplicity and small size of the dataset, capturing noise.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests will generalize better by averaging multiple decision trees, avoiding the bias of a single tree.</p>

        <h2>2. Wine Dataset</h2>
        <p><strong>Description:</strong> This dataset contains 13 features describing wine characteristics, with 178 samples. It’s a relatively small dataset, and overfitting may be an issue.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Trees tend to overfit when they are too deep, which can easily happen with the high dimensionality of the features.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests work better here by combining several shallow trees, reducing variance and overfitting.</p>

        <h2>3. Breast Cancer Wisconsin (Diagnostic) Dataset</h2>
        <p><strong>Description:</strong> This dataset has 30 features and 569 samples for predicting breast cancer type (malignant or benign).</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Decision trees can overfit because they might create deep trees to learn the intricate details of the data, leading to poor performance on unseen data.</p>
        <p><strong>Random Forest Advantage:</strong> By averaging predictions over many trees, random forests reduce the impact of outliers and noise.</p>

        <h2>4. Heart Disease Dataset</h2>
        <p><strong>Description:</strong> This dataset has 13 features related to patient attributes and outcomes (presence of heart disease).</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> With a limited number of instances and a relatively high number of features, decision trees can easily overfit, capturing the noise in the data.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests are more robust, as they create a collection of trees, each trained on a random subset of the data and features.</p>

        <h2>5. Adult (Census Income) Dataset</h2>
        <p><strong>Description:</strong> Contains demographic data for predicting whether a person earns more than $50K/year.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> The dataset is large and contains categorical variables. Decision trees may split deeply based on these variables and overfit to specific conditions.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests, by aggregating many trees, reduce the risk of overfitting, especially in high-dimensional datasets.</p>

        <h2>6. Titanic Dataset</h2>
        <p><strong>Description:</strong> Contains information about passengers on the Titanic, with features like age, class, and survival status.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Decision trees might overfit the smaller dataset by capturing too many specific relationships between features and the target variable.</p>
        <p><strong>Random Forest Advantage:</strong> By averaging multiple trees, random forests are less likely to overfit and can handle categorical and continuous features better.</p>

        <h2>7. Mushroom Dataset</h2>
        <p><strong>Description:</strong> A binary classification problem predicting whether a mushroom is edible or poisonous. This dataset has a large number of categorical features.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> The rich categorical nature of the dataset can easily lead to overfitting if the decision tree is allowed to grow too deep.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests handle such categorical data better by reducing the depth of individual trees and averaging out the results.</p>

        <h2>8. Diabetes Dataset</h2>
        <p><strong>Description:</strong> Used for predicting diabetes based on diagnostic measurements. The dataset has 8 features and 768 samples.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Decision trees might overfit, particularly with noisy data and small dataset sizes.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests reduce the chances of overfitting by combining multiple decision trees with different subsets of data.</p>

        <h2>9. Bank Marketing Dataset</h2>
        <p><strong>Description:</strong> Contains information about bank customers and their responses to marketing campaigns. It’s a large dataset with categorical features.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> A single decision tree may split too many times on the categorical variables, leading to overfitting.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests create multiple trees, reducing overfitting and improving generalization by considering multiple features and data splits.</p>

        <h2>10. Australian Weather Dataset</h2>
        <p><strong>Description:</strong> Contains weather data, including temperature, humidity, wind speed, etc., used to predict whether it will rain tomorrow.</p>
        <p><strong>Why Overfitting with Decision Trees:</strong> Decision trees may overfit on highly noisy or imbalanced weather data.</p>
        <p><strong>Random Forest Advantage:</strong> Random forests reduce overfitting by combining multiple decision trees, especially when dealing with noisy or complex features.</p>
        
        <h2>Conclusion</h2>
        <p>While decision trees may overfit on smaller, complex datasets or those with a high number of features, random forests tend to provide a better generalization by averaging over multiple trees, making them more robust in many practical situations.</p>
    </section>

    <!-- Counter Section -->
    <div id="counterSection">
        <p><strong>Page Load Counter: </strong><span id="counterDisplay">0</span></p>
    </div>
</body>
</html>
